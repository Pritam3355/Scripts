# -*- coding: utf-8 -*-
"""hospital_name_scrapper.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14RkVWO7L6L9359ovWFTf3NqLcEuUSGeQ
"""

from urllib.request import Request,urlopen
from bs4 import BeautifulSoup 
import csv,time,datetime,os
from urllib.parse import urljoin
import pandas as pd

url_h = 'https://www.medicineindia.org/hospitals-in-india'

#remove any file stored in current directory with same name

if os.path.exists('hospitals.csv'):
	os.remove('hospitals.csv')
	print('File Found,Deleted.')
else:
	pass

#open file in write mode
f = open('hospitals.csv','w',newline='')
#create a csv writer object to read & write sequence
w = csv.writer(f)

#send request & collect the table data in raw html format
#use the network tab in inspect element of any web page to get your browser headers
request = Request(url=url_h,headers={'User-Agent': 'Mozilla/5.0'})  
soup = BeautifulSoup(urlopen(request).read(),features="html5lib")
tbody = soup.find('table',{"class":"table table-striped"})

# table format

# <table class="table table-striped">
#   <tbody><th>table heading</th></tbody>
# 	<tbody>
# 		<tr>
# 			<td>date,open,high,low,%change value</td>
# 		</tr>
# 	</tbody>
# <table>

#storing the heading
for row in tbody.find_all('tr'):
  headings=row.find_all('th')
  headings=[x.text.strip() for x in headings]
  w.writerows(headings)

#storing the table data
for row in tbody.find_all('tr'):    #inside <tbody> go through each <tr>
	cols = row.find_all('td')       #find all <td> inside <tr>
	cols = [x.text.strip() for x in cols] #collect text inside <td>
	#no data present only column names are present 
	if cols == []:                         
		pass
	#data column , store them row-wise
	else:                                   
		w.writerow(cols)

df = pd.read_csv('hospitals.csv')
print("New File loaded")
df.head(10)

# we need 1st 3 columns and all rows starting from , as there is some overlap beause there are 2 <tbody> & both has <tr>
df=df.iloc[2:,0:3]
#renaming columns
df.columns=['Hospital Name','State','City']

df.to_csv('hospitals_list.csv',index=False)

print("Data stored in hospitals_list.csv")

df_tmp=pd.read_csv('/content/hospitals.csv')
df_tmp.head(5)

df=pd.read_csv('/content/hospitals_list.csv',index_col=False)
df.head(10)

df.nunique() # count of unique names

df.shape

pd.pandas.set_option("display.max_rows",None)
pd.pandas.set_option("display.max_columns",None)

gb=df.groupby(['State','City']).apply(lambda x: x[:])
gb.head(10)
# for key,value in gb:
#   print(gb.get_group(key),'\n')

print(f"Total duplicate count {df[df.duplicated()].count()}")
df[df.duplicated()]

df[df['Hospital Name']=='Anand Hospital'] # lets check the duplicates

df=df[~df.duplicated()].reset_index() # select unique hospitals

df[df['Hospital Name']=='Anand Hospital']

df_wb=df.loc[df['State']=='West Bengal']
gb_wb=df_wb.groupby('City').apply(lambda x: x[:])
gb_wb



# In[ ]:


df.State.value_counts()


# In[4]:


df_mod=df.loc[df['State']=='West Bengal']
df_mod.State.value_counts()


# In[5]:


df_hg=df_mod.loc[df_mod['City']=='Hooghly']
df_hw=df_mod.loc[df_mod['City']=='Howrah']
df_kl=df_mod.loc[df_mod['City']=='Kolkata']


# In[9]:


df1_hg = df_hg[['Name','Address','Phone']]
df1_hw = df_hw[['Name','Address','Phone']]
df1_kl = df_kl[['Name','Address','Phone']]
df1_hg.head(2)


# In[10]:


df1_hg.to_csv('Hooghly.csv',index=False)
df1_hw.to_csv('Howrah.csv',index=False)
df1_kl.to_csv('Kolkata.csv',index=False)


# In[ ]:





